{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e216b6a-4beb-4d9e-b403-6c8a363d4af3",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a64c2a-0a17-4c86-a91c-7f3590735eb1",
   "metadata": {},
   "source": [
    "The overall goal of this study is to give a general idea or step by step how to approach a Data Science or Machine Learning Problem from 0, that means from obtaining the Data till the model validation. \n",
    "\n",
    "The data of interest selected for this project were the housing market of Colombia, with the following features:\n",
    "\n",
    "* Number of bedrooms and bathrooms.\n",
    "* Area: area of the property, in m2.\n",
    "* Price: price of the property, in colombian pesos.\n",
    "* Location: city where the offer is available.\n",
    "* Type of property: house or apartment.\n",
    "* Stratum : number that classifies the zones that receives public services, used to asign differents money grants in them to low income families and tax high income ones. Stratums from 1 to 3 receive help and from 4 to 6 are taxed. This classification will be deleted by 2026.\n",
    "\n",
    "Because the Stratum classification will be erased, the other goal from the study is to label the data in new categories regardless that number; in other words, an unsupervised learning task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b86a9c-a2bd-4646-8691-8adfb6dd8002",
   "metadata": {},
   "source": [
    "# Data mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bce7333-7a40-4f9b-a8c6-8146a8e2751d",
   "metadata": {},
   "source": [
    "### Obtaining links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f768c8a-e5eb-4d05-aa7b-be2c73129176",
   "metadata": {},
   "source": [
    "Data was obtained from the website https://www.properati.com.co/ (base url). It's one of the places where is possible to find information about prices market in Colombia. \n",
    "\n",
    "First of all, it was built a script to gather all the links from the properties of the cities of interest in the study. To achieve that an inspection of the links and the code of the website was done, using the next aparment as an example:\n",
    "\n",
    "* view-source:https://www.properati.com.co/s/barranquilla-atlantico/apartamento/venta\n",
    "* https://www.properati.com.co/proyecto/14032-32-e557-f87edff6468c-7bb864f-88dd-4430\n",
    "\n",
    "By analyzing the source code of the list of properties of interests and the link of one of them it can be noticed that they have an unique identifier called 'data-idanuncio', and with this attribute there is another one called 'data-href'. Therefore, if you combine the base url and the data-href is possible to build all the links of all the properties.\n",
    "\n",
    "All the above was possible because the libraries requests, BeautifulSoup, time and pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df41cf02-23c2-402e-b723-ca80088b6163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "948ad536-b0fc-4f8c-8675-8798e8da5997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Pandas display option to show the full URL\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0e922be-92ae-4f99-a144-c247bf31962e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URLs\n",
    "base_url = \"https://www.properati.com.co\"\n",
    "url = \"https://www.properati.com.co/s/barranquilla-atlantico/apartamento/venta\"\n",
    "\n",
    "# Send an HTTP request to get the content of the page\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all elements with the data-idanuncio \n",
    "    listings = soup.find_all(attrs={\"data-idanuncio\": True})\n",
    "    \n",
    "    # List to store the links built\n",
    "    full_hrefs = []\n",
    "    \n",
    "    for listing in listings:\n",
    "        # Get the  data-href attribute\n",
    "        data_href = listing.get('data-href')\n",
    "        \n",
    "        if data_href:  # If data-href exists, build the full URL\n",
    "            full_url = base_url + data_href\n",
    "            full_hrefs.append(full_url)\n",
    "    \n",
    "    # Create a pandas DataFrame with the full URLs\n",
    "    df_1 = pd.DataFrame(full_hrefs, columns=['URL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63da5f62-e064-40a3-8a51-0bb322376d12",
   "metadata": {},
   "source": [
    "The previous script worked only for the first list of the properties of each city. If you move through the pages, it can be noticed the website adds a '/' with a number 'x', where x is the digit of the list, for example:\n",
    "\n",
    "* https://www.properati.com.co/s/barranquilla-atlantico/apartamento/venta/2\n",
    "\n",
    "Hence, the code was adapted to loop through all the digits that are available in the website for each city (this had to be checked manually). The numbers used were:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ac693032-5b6e-4061-a17d-665e764fcdd9",
   "metadata": {},
   "source": [
    "1.Bogota          apartments : 167, houses : 167 \n",
    "2.Cali            apartments : 167, houses : 123  -\n",
    "3.Medellin        apartments : 167, houses : 167  \n",
    "4.Barranquilla    apartments : 167, houses : 167  \n",
    "5.Cartagena       apartments : 167, houses : 44   \n",
    "6.Pereira         apartments : 127, houses : 154  \n",
    "7.Manizales       apartments : 122, houses : 97    \n",
    "8.Armenia         apartments : 83,  houses : 46    \n",
    "9.Cucuta          apartments : 44,  houses : 72    \n",
    "10.Santa Marta    apartments : 54,  houses : 31   \n",
    "11.Bucaramanga    apartments : 57,  houses : 14   \n",
    "12.Monteria       apartments : 28,  houses : 53    \n",
    "13.Popayan        apartments : 13,  houses : 22    \n",
    "14.Villavicencio  apartments : 14,  houses : 37   \n",
    "15.Ibague         apartments : 31,  houses : 17    \n",
    "16.Pasto          apartments : 1,   houses : 1\n",
    "17.Neiva          apartments : 7,   houses : 9      \n",
    "18.Valledupar     apartments : 2,   houses : 2      \n",
    "19.Sincelejo      apartments : 3,   houses : 5      \n",
    "20.Tunja          apartments : 9,   houses : 9     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed10cb74-92a1-4138-b011-0e00dea03410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base URL\n",
    "base_url = \"https://www.properati.com.co\"\n",
    "\n",
    "# Define the target URL pattern (replace page number dynamically)\n",
    "url_pattern = \"https://www.properati.com.co/s/barranquilla-atlantico/apartamento/venta/{}\"\n",
    "\n",
    "# List to store the full href values\n",
    "full_hrefs = []\n",
    "\n",
    "# Loop through pages \n",
    "for page_num in range(2, 168):  # remember to adapt it according to the City\n",
    "    \n",
    "    # Construct the URL for the current page\n",
    "    url = url_pattern.format(page_num)\n",
    "    \n",
    "    # Send an HTTP request to get the content of the page\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find all elements with the data-idanuncio attribute\n",
    "        listings = soup.find_all(attrs={\"data-idanuncio\": True})\n",
    "        \n",
    "        for listing in listings:\n",
    "            # Extract the value of the data-href attribute\n",
    "            data_href = listing.get('data-href')\n",
    "            \n",
    "            if data_href:  # If data-href exists, store the full URL\n",
    "                full_url = base_url + data_href\n",
    "                full_hrefs.append(full_url)\n",
    "        \n",
    "        print(f\"Successfully scraped page {page_num}\")\n",
    "        \n",
    "        # Add a short delay between requests to avoid overwhelming the server or triggering the captcha bot\n",
    "        time.sleep(1)\n",
    "    else:\n",
    "        print(f\"Failed to retrieve page {page_num}. HTTP Status Code: {response.status_code}\")\n",
    "\n",
    "# Create a pandas DataFrame with the full URLs\n",
    "df = pd.DataFrame(full_hrefs, columns=['URL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05a4f93-b59f-499e-9796-741c70275760",
   "metadata": {},
   "source": [
    "Finally, the links obtained were merged and saved in a csv file. All the URLs for each city can be found in the Data Links folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52c194b-64fe-4b73-bb5e-2606c1794d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the links dataframes\n",
    "links = pd.concat([df, df_1], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a347cb-d203-4fa6-a8aa-11884848f937",
   "metadata": {},
   "outputs": [],
   "source": [
    "links.to_csv('properati_listings_tunja_apt.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd43bbcd-5fbc-4a1f-b18b-6a058570e6b9",
   "metadata": {},
   "source": [
    "### Obtaining data from each link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6be447c-055f-4232-974b-e054f8dc4c9e",
   "metadata": {},
   "source": [
    "A similar process was done to obtain the relevant information of each property. An example link was analyzed and could be noticed that the interest data is present in the classes location, place_details, place_features and price. With that in mind, the next script was built to loop through each URL to extract the data of each apartment or house:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3353e591-f92a-4bf4-b131-baf7c3443107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657b6c75-707e-49ed-a75d-fac57c396418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Selenium \n",
    "driver = webdriver.Chrome()  \n",
    "\n",
    "# Open csv file of interest\n",
    "df = pd.read_csv('properati_listings_armenia_apt.csv')\n",
    "\n",
    "# Create a list to store the extracted data\n",
    "data = []\n",
    "\n",
    "# Loop through each URL \n",
    "for index, row in df.iterrows():\n",
    "    property_url =  row['URL']\n",
    "    driver.get(property_url)\n",
    "    time.sleep(5) # Adjust this depending on your internet speed, if it is too high it will trigger captcha!\n",
    "    \n",
    "    # Extract location, bedrooms, area, bathrooms, area and price according to website\n",
    "    location = driver.find_element(By.CLASS_NAME, \"location\").text.strip() if driver.find_elements(By.CLASS_NAME, \"location\") else 'N/A'\n",
    "    \n",
    "    place_details = driver.find_elements(By.CLASS_NAME, \"place-details--all-elements-showing\")\n",
    "    if place_details:\n",
    "        bedrooms = place_details[0].find_element(By.XPATH, './/div[@data-test=\"bedrooms-value\"]').text.strip() if place_details[0].find_elements(By.XPATH, './/div[@data-test=\"bedrooms-value\"]') else 'N/A'\n",
    "        bathrooms = place_details[0].find_element(By.XPATH, './/div[@data-test=\"full-bathrooms-value\"]').text.strip() if place_details[0].find_elements(By.XPATH, './/div[@data-test=\"full-bathrooms-value\"]') else 'N/A'\n",
    "        area = place_details[0].find_element(By.XPATH, './/div[@data-test=\"area-value\"]').text.strip() if place_details[0].find_elements(By.XPATH, './/div[@data-test=\"area-value\"]') else 'N/A'\n",
    "    else:\n",
    "        bedrooms = bathrooms = area = 'N/A'\n",
    "    \n",
    "    place_features = driver.find_elements(By.CLASS_NAME, \"place-features\")\n",
    "    if place_features:\n",
    "        property_type = place_features[0].find_element(By.XPATH, './/span[@data-test=\"property-type-value\"]').text.strip() if place_features[0].find_elements(By.XPATH, './/span[@data-test=\"property-type-value\"]') else 'N/A'\n",
    "        stratum = place_features[0].find_element(By.XPATH, './/span[@data-test=\"stratum-value\"]').text.strip() if place_features[0].find_elements(By.XPATH, './/span[@data-test=\"stratum-value\"]') else 'N/A'\n",
    "    else:\n",
    "        property_type = stratum = 'N/A'\n",
    "    \n",
    "    price = driver.find_element(By.XPATH, '//*[@data-test=\"listing-price\"]').text.strip() if driver.find_elements(By.XPATH, '//*[@data-test=\"listing-price\"]') else 'N/A'\n",
    "    \n",
    "    # Append the extracted data \n",
    "    data.append({\n",
    "        'URL': property_url,\n",
    "        'Location': location,\n",
    "        'Bedrooms': bedrooms,\n",
    "        'Bathrooms': bathrooms,\n",
    "        'Area': area,\n",
    "        'Property Type': property_type,\n",
    "        'Stratum': stratum,\n",
    "        'Price': price\n",
    "    })\n",
    "    \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85705da2-bf6f-4b42-8bb4-5efa1744a589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store extracted data as a pandas DataFrame\n",
    "properties_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4381e9cf-40a5-4b94-8eca-e29723a64156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the extracted data to a CSV file\n",
    "properties_df.to_csv('barranquilla_apt.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac985db6-7d57-4a97-b850-0d36a6d78948",
   "metadata": {},
   "source": [
    "All the above was possible thanks to the Selenium library. Trying to use BeautifulSoup here trigerred the captcha from the website. Finally, all the extracted data can be found in the Data cities no cleaned library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00380d77-c48d-4889-ae36-8354ae8828e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
